{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf5d11-5ba5-41ba-aab4-c2cf9c4d3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
    "\n",
    "'''\n",
    "->Ensemble Learning in machine learning refers to a technique where multiple models (often called \"learners\" or \"base models\") are combined to solve a particular problem‚Äîtypically to improve overall prediction performance compared to individual models.\n",
    "\n",
    "Common Ensemble Methods:\n",
    "Bagging (Bootstrap Aggregating):\n",
    "Trains multiple versions of the same model on different random subsets of the data.\n",
    "Final prediction: average (regression) or majority vote (classification).\n",
    "Example: Random Forest.\n",
    "\n",
    "Boosting:\n",
    "Builds models sequentially, where each new model focuses on correcting the errors of the previous ones.\n",
    "Final prediction: weighted sum or majority vote.\n",
    "Example: AdaBoost, Gradient Boosting, XGBoost.\n",
    "\n",
    "Stacking (Stacked Generalization):\n",
    "Trains multiple diverse models and then combines their outputs using a meta-model.\n",
    "More flexible but more complex to train.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5927476-65d1-485a-84cf-bb92e5b69539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the difference between Bagging and Boosting?\n",
    "\n",
    "'''\n",
    "->| Feature               | **Bagging**                                                                | **Boosting**                                                                   |\n",
    "| --------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| üîÑ **Model Training** | Models are trained **independently** in **parallel**                       | Models are trained **sequentially**, each correcting the previous              |\n",
    "| üóÇÔ∏è **Data Sampling** | Uses **random subsets** of the data (with replacement ‚Äî bootstrap samples) | Uses **all data**, but gives more weight to previously misclassified points    |\n",
    "| üß† **Focus**          | Reduces **variance** (helps with overfitting)                              | Reduces **bias** (helps with underfitting)                                     |\n",
    "| ‚öñÔ∏è **Weighting**      | All models have **equal weight** in the final prediction                   | Models are **weighted** by accuracy (more accurate models have more influence) |\n",
    "| ‚öôÔ∏è **Complexity**     | Easier to parallelize, often faster                                        | More complex an                                                                |\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9726b-782c-4224-9518-6f987eb30273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?.\n",
    "\n",
    "'''\n",
    "->Bootstrap sampling is a statistical technique used to create multiple datasets by randomly sampling with replacement from an original dataset. It's a key component in Bagging methods like Random Forest.\n",
    "\n",
    " Role in Bagging (e.g., Random Forest):\n",
    "In Bagging, especially Random Forest:\n",
    "Multiple bootstrap samples are generated from the original training data.\n",
    "A separate model (e.g., decision tree) is trained on each bootstrap sample.\n",
    "Predictions from all models are combined:\n",
    "Averaged for regression tasks.\n",
    "Majority vote for classification tasks.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7d6cd-79f6-4e37-bb37-01376fc0fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
    "\n",
    "'''\n",
    "->Out-of-Bag (OOB) samples and the OOB score are concepts used in Bagging methods like Random Forest to evaluate model performance without needing a separate validation set or cross-validation\n",
    "\n",
    "How Is the OOB Score Used?\n",
    "Train each model (e.g., decision tree in Random Forest) on its bootstrap sample.\n",
    "For every data point, collect predictions from the models that did NOT see that point (i.e., for which it was an OOB sample).\n",
    "Aggregate those OOB predictions (e.g., majority vote or average).\n",
    "\n",
    "Why OOB Evaluation Is Useful:\n",
    "Acts like built-in cross-validation.\n",
    "No need to hold out a separate test or validation set.\n",
    "Provides an unbiased estimate of model performance on unseen data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27018a-f40d-4853-833f-cb734cd6c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
    "\n",
    "'''\n",
    "->1. Decision Tree Feature Importance:\n",
    "How it's computed:\n",
    "A single decision tree calculates feature importance based on the reduction in impurity (like Gini impurity or entropy) each time a feature is used to split the data.\n",
    "\n",
    "Pros:\n",
    "Easy to interpret.\n",
    "Fast to compute.\n",
    "\n",
    "Cons:\n",
    "High variance ‚Äî sensitive to training data.\n",
    "Can be biased toward features with more levels (e.g., categorical variables with many categories).\n",
    "\n",
    " 2. Random Forest Feature Importance:\n",
    "How it's computed:\n",
    "Aggregates feature importances across all trees in the forest.\n",
    "Each tree computes its own importance (based on impurity reduction), and the values are averaged and normalized.\n",
    "\n",
    "Optional advanced method:\n",
    "Permutation Importance: Shuffle each feature and observe how model performance changes ‚Äî a drop indicates that the feature was important.\n",
    "\n",
    "Pros:\n",
    "More stable and generalizable due to averaging.\n",
    "Reduces the effect of overfitting.\n",
    "\n",
    "Cons:\n",
    "Less interpretable than a single tree.\n",
    "Still somewhat biased toward features with many unique values (unless using permutation importance).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65d777f-faa9-4677-a0b5-7577bd2aa4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Important Features:\n",
      "                 Feature  Importance\n",
      "23            worst area    0.139357\n",
      "27  worst concave points    0.132225\n",
      "7    mean concave points    0.107046\n",
      "20          worst radius    0.082848\n",
      "22       worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "#Write a Python program to:‚óè Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()‚óè Train a Random Forest Classifier‚óè Print the top 5 most important features based on feature importance scores.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for easy viewing\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort by importance and get top 5\n",
    "top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
    "\n",
    "# Print the top 5 features\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eade50cc-0e0a-410e-bde1-54dd70e5039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of single Decision Tree: 100.00%\n",
      "Accuracy of Bagging Classifier: 100.00%\n",
      "Accuracy Improvement: 0.00%\n"
     ]
    }
   ],
   "source": [
    "#Write a Python program to:‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset‚óè Evaluate its accuracy and compare with a single Decision Tree\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a single Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_preds = dt_model.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
    "\n",
    "# Train a Bagging Classifier with Decision Trees as base estimators\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_preds = bagging_model.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy of single Decision Tree: {:.2f}%\".format(dt_accuracy * 100))\n",
    "print(\"Accuracy of Bagging Classifier: {:.2f}%\".format(bagging_accuracy * 100))\n",
    "print(\"Accuracy Improvement: {:.2f}%\".format((bagging_accuracy - dt_accuracy) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d43f5c4f-29d3-4e5b-932a-29815ecd6175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Parameters: {'max_depth': None, 'n_estimators': 10}\n",
      "Final Accuracy: 0.9111\n"
     ]
    }
   ],
   "source": [
    "#: Write a Python program to:‚óè Train a Random Forest Classifier‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV‚óè Print the best parameters and final accuracy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset (replace with your own if needed)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],    # Number of trees\n",
    "    'max_depth': [None, 5, 10, 20]          # Max depth of each tree\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,           # 5-fold cross-validation\n",
    "    n_jobs=-1,      # Use all processors\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on test set with best estimator\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Final accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a47947a-3556-4c04-8f2a-c5db348d93ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of Bagging Regressor: 0.2573\n",
      "Mean Squared Error of Random Forest Regressor: 0.2573\n",
      "MSE Difference (Bagging - RF): 0.0000\n"
     ]
    }
   ],
   "source": [
    "#Write a Python program to:‚óè Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset‚óè Compare their Mean Squared Errors (MSE)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Bagging Regressor with Decision Trees\n",
    "bagging_reg = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "bagging_preds = bagging_reg.predict(X_test)\n",
    "bagging_mse = mean_squared_error(y_test, bagging_preds)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "rf_reg.fit(X_train, y_train)\n",
    "rf_preds = rf_reg.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_preds)\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Squared Error of Bagging Regressor: {:.4f}\".format(bagging_mse))\n",
    "print(\"Mean Squared Error of Random Forest Regressor: {:.4f}\".format(rf_mse))\n",
    "\n",
    "# Optional: Difference\n",
    "print(\"MSE Difference (Bagging - RF): {:.4f}\".format(bagging_mse - rf_mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ac63f-f23f-48fe-9d30-cc5440c6644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.You decide to use ensemble techniques to increase model performance.Explain your step-by-step approach to:‚óè Choose between Bagging or Boosting‚óè Handle overfitting‚óè Select base models‚óè Evaluate performance using cross-validation‚óè Justify how ensemble learning improves decision-making in this real-world context.\n",
    "\n",
    "'''\n",
    "->1. Choosing Between Bagging and Boosting\n",
    "Bagging (e.g., Random Forest):\n",
    "Good if your base models are high variance (like deep decision trees).\n",
    "Works by training many independent models on random subsets of data, then averaging/voting.\n",
    "\n",
    "Boosting (e.g., Gradient Boosting, XGBoost, LightGBM):\n",
    "Focuses on sequentially correcting mistakes from previous models.\n",
    "Can achieve higher predictive accuracy but more prone to overfitting if not tuned well.\n",
    "Often better on imbalanced or complex datasets, common in financial defaults.\n",
    "\n",
    "2. Handling Overfitting\n",
    "Use cross-validation (e.g., stratified K-fold) to monitor generalization.\n",
    "\n",
    "For Bagging:\n",
    "Limit tree depth.\n",
    "Use more trees to reduce variance.\n",
    "\n",
    "3. Selecting Base Models\n",
    "Typically, decision trees are used as base learners because:\n",
    "They capture nonlinear relationships and interactions well.\n",
    "Easy to ensemble and interpret.\n",
    "\n",
    "4. Evaluating Performance Using Cross-Validation\n",
    "Use Stratified K-Fold Cross-Validation to maintain the proportion of default vs. non-default cases in each fold.\n",
    "Evaluate multiple metrics, including:\n",
    "AUC-ROC (discrimination ability)\n",
    "Precision-Recall AUC (especially if classes are imbalanced)\n",
    "\n",
    "5. How Ensemble Learning Improves Decision-Making in This Context\n",
    "Higher accuracy and robustness: Ensemble models combine multiple weak learners to reduce errors from individual models, improving prediction reliability ‚Äî critical in high-stakes loan decisions.\n",
    "Capturing complex patterns: Ensembles can model nonlinearities and interactions in customer demographics and transaction histories that single models may miss.\n",
    "Reducing risk of costly mistakes: Better prediction reduces false negatives (missing defaults) and false positives (denying credit to good customers), optimizing risk and customer satisfaction.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f46a3-7269-495f-91e3-046ee0830516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8edc2-5230-4cc6-b122-47800a5fed65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33280edb-c00f-42fc-a6af-e9793e3f82bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
